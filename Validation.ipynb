{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd26ceb7-c986-4651-94d6-f0b1f0c8e9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in /home/mzc/anaconda3/lib/python3.11/site-packages (8.3.34)\n",
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.3.38-py3-none-any.whl.metadata (35 kB)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /home/mzc/anaconda3/lib/python3.11/site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /home/mzc/anaconda3/lib/python3.11/site-packages (from ultralytics) (3.8.0)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /home/mzc/anaconda3/lib/python3.11/site-packages (from ultralytics) (4.10.0.84)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /home/mzc/anaconda3/lib/python3.11/site-packages (from ultralytics) (10.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/mzc/anaconda3/lib/python3.11/site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in /home/mzc/anaconda3/lib/python3.11/site-packages (from ultralytics) (2.31.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/mzc/anaconda3/lib/python3.11/site-packages (from ultralytics) (1.11.4)\n",
      "Requirement already satisfied: torch>=1.8.0 in /home/mzc/anaconda3/lib/python3.11/site-packages (from ultralytics) (2.5.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /home/mzc/anaconda3/lib/python3.11/site-packages (from ultralytics) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /home/mzc/anaconda3/lib/python3.11/site-packages (from ultralytics) (4.65.0)\n",
      "Requirement already satisfied: psutil in /home/mzc/anaconda3/lib/python3.11/site-packages (from ultralytics) (5.9.0)\n",
      "Requirement already satisfied: py-cpuinfo in /home/mzc/anaconda3/lib/python3.11/site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /home/mzc/anaconda3/lib/python3.11/site-packages (from ultralytics) (2.1.4)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /home/mzc/anaconda3/lib/python3.11/site-packages (from ultralytics) (0.12.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /home/mzc/anaconda3/lib/python3.11/site-packages (from ultralytics) (2.0.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/mzc/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/mzc/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/mzc/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/mzc/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mzc/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/mzc/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/mzc/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/mzc/anaconda3/lib/python3.11/site-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/mzc/anaconda3/lib/python3.11/site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mzc/anaconda3/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mzc/anaconda3/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mzc/anaconda3/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mzc/anaconda3/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2024.2.2)\n",
      "Requirement already satisfied: filelock in /home/mzc/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/mzc/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/mzc/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/mzc/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/mzc/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/mzc/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/mzc/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/mzc/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/mzc/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/mzc/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/mzc/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/mzc/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/mzc/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/mzc/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/mzc/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/mzc/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/mzc/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/mzc/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/mzc/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/mzc/anaconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/mzc/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/mzc/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
      "Downloading ultralytics-8.3.38-py3-none-any.whl (896 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m896.3/896.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ultralytics\n",
      "  Attempting uninstall: ultralytics\n",
      "    Found existing installation: ultralytics 8.3.34\n",
      "    Uninstalling ultralytics-8.3.34:\n",
      "      Successfully uninstalled ultralytics-8.3.34\n",
      "Successfully installed ultralytics-8.3.38\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics --upgrade\n",
    "import os\n",
    "# Import necessary libraries\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05a01a00-ec8e-4419-a292-3da27560b2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.38 üöÄ Python-3.11.7 torch-2.5.1+cu124 CUDA:0 (NVIDIA TITAN Xp COLLECTORS EDITION, 12189MiB)\n",
      "YOLO11n-seg summary (fused): 265 layers, 2,834,958 parameters, 0 gradients, 10.2 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11Att\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/valid/images/tree12_00700_jpg.rf.79df4c55f73b4b9735fa1d83a09b1a92.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/valid/images/tree14_00500_jpg.rf.a4b19cb8c74fec773ca46434642db08f.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/valid/images/tree6_01000_jpg.rf.49bd8f6c55b97b57cad26a96e1ddecb9.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/valid/images/tree7_00600_jpg.rf.3fd015c4f0bc7b60680d45d2f7901404.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/valid/images/tree8_00700_jpg.rf.82d15fb1e082da1e6df9993535ed5011.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/valid/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         78       3558      0.877      0.817      0.874      0.681      0.844      0.777      0.827      0.509\n",
      "                Branch         78       2651      0.856      0.731      0.814      0.569      0.795      0.664       0.73      0.361\n",
      "                 Trunk         78        907      0.897      0.903      0.933      0.794      0.894      0.891      0.924      0.656\n",
      "Speed: 1.5ms preprocess, 3.8ms inference, 0.0ms loss, 5.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val\u001b[0m\n",
      "Validation Metrics for Bounding Box:\n",
      "mAP50-95: 0.6813040914649258\n",
      "mAP50: 0.87384024773062\n",
      "mAP75: 0.7620903550641998\n",
      "Validation Metrics for Segmentation Mask:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SegmentMetrics' object has no attribute 'mask'. See valid attributes below.\n\n    Calculates and aggregates detection and segmentation metrics over a given set of classes.\n\n    Args:\n        save_dir (Path): Path to the directory where the output plots should be saved. Default is the current directory.\n        plot (bool): Whether to save the detection and segmentation plots. Default is False.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n        names (list): List of class names. Default is an empty list.\n\n    Attributes:\n        save_dir (Path): Path to the directory where the output plots should be saved.\n        plot (bool): Whether to save the detection and segmentation plots.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n        names (list): List of class names.\n        box (Metric): An instance of the Metric class to calculate box detection metrics.\n        seg (Metric): An instance of the Metric class to calculate mask segmentation metrics.\n        speed (dict): Dictionary to store the time taken in different phases of inference.\n\n    Methods:\n        process(tp_m, tp_b, conf, pred_cls, target_cls): Processes metrics over the given set of predictions.\n        mean_results(): Returns the mean of the detection and segmentation metrics over all the classes.\n        class_result(i): Returns the detection and segmentation metrics of class `i`.\n        maps: Returns the mean Average Precision (mAP) scores for IoU thresholds ranging from 0.50 to 0.95.\n        fitness: Returns the fitness scores, which are a single weighted combination of metrics.\n        ap_class_index: Returns the list of indices of classes used to compute Average Precision (AP).\n        results_dict: Returns the dictionary containing all the detection and segmentation metrics and fitness score.\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP75: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mbox\u001b[38;5;241m.\u001b[39mmap75\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Metrics for Segmentation Mask:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP50-95: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmap\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP50: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmap50\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP75: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmap75\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/utils/__init__.py:221\u001b[0m, in \u001b[0;36mSimpleClass.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Custom attribute access error message with helpful information.\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. See valid attributes below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SegmentMetrics' object has no attribute 'mask'. See valid attributes below.\n\n    Calculates and aggregates detection and segmentation metrics over a given set of classes.\n\n    Args:\n        save_dir (Path): Path to the directory where the output plots should be saved. Default is the current directory.\n        plot (bool): Whether to save the detection and segmentation plots. Default is False.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n        names (list): List of class names. Default is an empty list.\n\n    Attributes:\n        save_dir (Path): Path to the directory where the output plots should be saved.\n        plot (bool): Whether to save the detection and segmentation plots.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n        names (list): List of class names.\n        box (Metric): An instance of the Metric class to calculate box detection metrics.\n        seg (Metric): An instance of the Metric class to calculate mask segmentation metrics.\n        speed (dict): Dictionary to store the time taken in different phases of inference.\n\n    Methods:\n        process(tp_m, tp_b, conf, pred_cls, target_cls): Processes metrics over the given set of predictions.\n        mean_results(): Returns the mean of the detection and segmentation metrics over all the classes.\n        class_result(i): Returns the detection and segmentation metrics of class `i`.\n        maps: Returns the mean Average Precision (mAP) scores for IoU thresholds ranging from 0.50 to 0.95.\n        fitness: Returns the fitness scores, which are a single weighted combination of metrics.\n        ap_class_index: Returns the list of indices of classes used to compute Average Precision (AP).\n        results_dict: Returns the dictionary containing all the detection and segmentation metrics and fitness score.\n    "
     ]
    }
   ],
   "source": [
    "# VALIDATION ON DORMANT DATASET\n",
    "#Import necessary libraries\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "model_path = '/media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/Experiment/CBAM/N/train_cbam/weights/best.pt'\n",
    "data_path = '/media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/data.yaml'\n",
    "\n",
    "# Load the YOLO11n model\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# Validate the model using the new dataset\n",
    "results = model.val(data=data_path)\n",
    "\n",
    "# Print the validation metrics\n",
    "print(\"Validation Metrics for Bounding Box:\")\n",
    "print(f\"mAP50-95: {results.box.map}\")\n",
    "print(f\"mAP50: {results.box.map50}\")\n",
    "print(f\"mAP75: {results.box.map75}\")\n",
    "\n",
    "print(\"Validation Metrics for Segmentation Mask:\")\n",
    "print(f\"mAP50-95: {results.mask.map}\")\n",
    "print(f\"mAP50: {results.mask.map50}\")\n",
    "print(f\"mAP75: {results.mask.map75}\")\n",
    "\n",
    "# Assume results.speed contains the processing speed information\n",
    "print(f\"Processing Speeds: {results.speed}\")\n",
    "\n",
    "# Additional code to visualize results or save output can be added here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45805468-3cfd-404c-86c1-9d67ee9201e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.38 üöÄ Python-3.11.7 torch-2.5.1+cu124 CUDA:0 (NVIDIA TITAN Xp COLLECTORS EDITION, 12189MiB)\n",
      "YOLO11s-seg summary (fused): 265 layers, 10,067,590 parameters, 0 gradients, 35.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11Att\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/valid/images/tree12_00700_jpg.rf.79df4c55f73b4b9735fa1d83a09b1a92.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/valid/images/tree14_00500_jpg.rf.a4b19cb8c74fec773ca46434642db08f.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/valid/images/tree6_01000_jpg.rf.49bd8f6c55b97b57cad26a96e1ddecb9.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/valid/images/tree7_00600_jpg.rf.3fd015c4f0bc7b60680d45d2f7901404.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/valid/images/tree8_00700_jpg.rf.82d15fb1e082da1e6df9993535ed5011.jpg: 1 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         78       3558      0.895      0.848      0.903      0.738      0.876      0.792      0.847      0.528\n",
      "                Branch         78       2651       0.88      0.785      0.858       0.64      0.831      0.696      0.761      0.385\n",
      "                 Trunk         78        907      0.911       0.91      0.947      0.836      0.921      0.887      0.934      0.671\n",
      "Speed: 14.6ms preprocess, 6.9ms inference, 0.0ms loss, 2.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val2\u001b[0m\n",
      "Validation Metrics for Bounding Box:\n",
      "mAP50-95: 0.7376672024992643\n",
      "mAP50: 0.9026525547659074\n",
      "mAP75: 0.8211331314446957\n",
      "Validation Metrics for Segmentation Mask:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SegmentMetrics' object has no attribute 'mask'. See valid attributes below.\n\n    Calculates and aggregates detection and segmentation metrics over a given set of classes.\n\n    Args:\n        save_dir (Path): Path to the directory where the output plots should be saved. Default is the current directory.\n        plot (bool): Whether to save the detection and segmentation plots. Default is False.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n        names (list): List of class names. Default is an empty list.\n\n    Attributes:\n        save_dir (Path): Path to the directory where the output plots should be saved.\n        plot (bool): Whether to save the detection and segmentation plots.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n        names (list): List of class names.\n        box (Metric): An instance of the Metric class to calculate box detection metrics.\n        seg (Metric): An instance of the Metric class to calculate mask segmentation metrics.\n        speed (dict): Dictionary to store the time taken in different phases of inference.\n\n    Methods:\n        process(tp_m, tp_b, conf, pred_cls, target_cls): Processes metrics over the given set of predictions.\n        mean_results(): Returns the mean of the detection and segmentation metrics over all the classes.\n        class_result(i): Returns the detection and segmentation metrics of class `i`.\n        maps: Returns the mean Average Precision (mAP) scores for IoU thresholds ranging from 0.50 to 0.95.\n        fitness: Returns the fitness scores, which are a single weighted combination of metrics.\n        ap_class_index: Returns the list of indices of classes used to compute Average Precision (AP).\n        results_dict: Returns the dictionary containing all the detection and segmentation metrics and fitness score.\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP75: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mbox\u001b[38;5;241m.\u001b[39mmap75\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Metrics for Segmentation Mask:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP50-95: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmap\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP50: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmap50\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP75: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmap75\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/utils/__init__.py:221\u001b[0m, in \u001b[0;36mSimpleClass.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Custom attribute access error message with helpful information.\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. See valid attributes below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SegmentMetrics' object has no attribute 'mask'. See valid attributes below.\n\n    Calculates and aggregates detection and segmentation metrics over a given set of classes.\n\n    Args:\n        save_dir (Path): Path to the directory where the output plots should be saved. Default is the current directory.\n        plot (bool): Whether to save the detection and segmentation plots. Default is False.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n        names (list): List of class names. Default is an empty list.\n\n    Attributes:\n        save_dir (Path): Path to the directory where the output plots should be saved.\n        plot (bool): Whether to save the detection and segmentation plots.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n        names (list): List of class names.\n        box (Metric): An instance of the Metric class to calculate box detection metrics.\n        seg (Metric): An instance of the Metric class to calculate mask segmentation metrics.\n        speed (dict): Dictionary to store the time taken in different phases of inference.\n\n    Methods:\n        process(tp_m, tp_b, conf, pred_cls, target_cls): Processes metrics over the given set of predictions.\n        mean_results(): Returns the mean of the detection and segmentation metrics over all the classes.\n        class_result(i): Returns the detection and segmentation metrics of class `i`.\n        maps: Returns the mean Average Precision (mAP) scores for IoU thresholds ranging from 0.50 to 0.95.\n        fitness: Returns the fitness scores, which are a single weighted combination of metrics.\n        ap_class_index: Returns the list of indices of classes used to compute Average Precision (AP).\n        results_dict: Returns the dictionary containing all the detection and segmentation metrics and fitness score.\n    "
     ]
    }
   ],
   "source": [
    "# VALIDATION ON DORMANT DATASET\n",
    "#Import necessary libraries\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "model_path = '/media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/Experiment/CBAM/S/train_cbam/weights/best.pt'\n",
    "data_path = '/media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/data.yaml'\n",
    "\n",
    "# Load the YOLO11n model\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# Validate the model using the new dataset\n",
    "results = model.val(data=data_path)\n",
    "\n",
    "# Print the validation metrics\n",
    "print(\"Validation Metrics for Bounding Box:\")\n",
    "print(f\"mAP50-95: {results.box.map}\")\n",
    "print(f\"mAP50: {results.box.map50}\")\n",
    "print(f\"mAP75: {results.box.map75}\")\n",
    "\n",
    "print(\"Validation Metrics for Segmentation Mask:\")\n",
    "print(f\"mAP50-95: {results.mask.map}\")\n",
    "print(f\"mAP50: {results.mask.map50}\")\n",
    "print(f\"mAP75: {results.mask.map75}\")\n",
    "\n",
    "# Assume results.speed contains the processing speed information\n",
    "print(f\"Processing Speeds: {results.speed}\")\n",
    "\n",
    "# Additional code to visualize results or save output can be added here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84b7deae-5d14-4d09-a1dd-b3c3725cf100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.38 üöÄ Python-3.11.7 torch-2.5.1+cu124 CUDA:0 (NVIDIA TITAN Xp COLLECTORS EDITION, 12189MiB)\n",
      "YOLO11m-seg summary (fused): 330 layers, 22,336,854 parameters, 0 gradients, 123.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11Att\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/valid/images/tree12_00700_jpg.rf.79df4c55f73b4b9735fa1d83a09b1a92.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/valid/images/tree14_00500_jpg.rf.a4b19cb8c74fec773ca46434642db08f.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/valid/images/tree6_01000_jpg.rf.49bd8f6c55b97b57cad26a96e1ddecb9.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/valid/images/tree7_00600_jpg.rf.3fd015c4f0bc7b60680d45d2f7901404.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/valid/images/tree8_00700_jpg.rf.82d15fb1e082da1e6df9993535ed5011.jpg: 1 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         78       3558       0.94      0.898      0.935      0.819      0.903      0.842      0.886      0.588\n",
      "                Branch         78       2651      0.922      0.846      0.901      0.744      0.859      0.757      0.813      0.446\n",
      "                 Trunk         78        907      0.958      0.949      0.969      0.893      0.948      0.927       0.96       0.73\n",
      "Speed: 1.8ms preprocess, 9.3ms inference, 0.0ms loss, 1.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val3\u001b[0m\n",
      "Validation Metrics for Bounding Box:\n",
      "mAP50-95: 0.8186637634186618\n",
      "mAP50: 0.9351570157538479\n",
      "mAP75: 0.8861069047408071\n",
      "Validation Metrics for Segmentation Mask:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SegmentMetrics' object has no attribute 'mask'. See valid attributes below.\n\n    Calculates and aggregates detection and segmentation metrics over a given set of classes.\n\n    Args:\n        save_dir (Path): Path to the directory where the output plots should be saved. Default is the current directory.\n        plot (bool): Whether to save the detection and segmentation plots. Default is False.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n        names (list): List of class names. Default is an empty list.\n\n    Attributes:\n        save_dir (Path): Path to the directory where the output plots should be saved.\n        plot (bool): Whether to save the detection and segmentation plots.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n        names (list): List of class names.\n        box (Metric): An instance of the Metric class to calculate box detection metrics.\n        seg (Metric): An instance of the Metric class to calculate mask segmentation metrics.\n        speed (dict): Dictionary to store the time taken in different phases of inference.\n\n    Methods:\n        process(tp_m, tp_b, conf, pred_cls, target_cls): Processes metrics over the given set of predictions.\n        mean_results(): Returns the mean of the detection and segmentation metrics over all the classes.\n        class_result(i): Returns the detection and segmentation metrics of class `i`.\n        maps: Returns the mean Average Precision (mAP) scores for IoU thresholds ranging from 0.50 to 0.95.\n        fitness: Returns the fitness scores, which are a single weighted combination of metrics.\n        ap_class_index: Returns the list of indices of classes used to compute Average Precision (AP).\n        results_dict: Returns the dictionary containing all the detection and segmentation metrics and fitness score.\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP75: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mbox\u001b[38;5;241m.\u001b[39mmap75\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Metrics for Segmentation Mask:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP50-95: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmap\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP50: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmap50\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP75: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmap75\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/utils/__init__.py:221\u001b[0m, in \u001b[0;36mSimpleClass.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Custom attribute access error message with helpful information.\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. See valid attributes below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SegmentMetrics' object has no attribute 'mask'. See valid attributes below.\n\n    Calculates and aggregates detection and segmentation metrics over a given set of classes.\n\n    Args:\n        save_dir (Path): Path to the directory where the output plots should be saved. Default is the current directory.\n        plot (bool): Whether to save the detection and segmentation plots. Default is False.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n        names (list): List of class names. Default is an empty list.\n\n    Attributes:\n        save_dir (Path): Path to the directory where the output plots should be saved.\n        plot (bool): Whether to save the detection and segmentation plots.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n        names (list): List of class names.\n        box (Metric): An instance of the Metric class to calculate box detection metrics.\n        seg (Metric): An instance of the Metric class to calculate mask segmentation metrics.\n        speed (dict): Dictionary to store the time taken in different phases of inference.\n\n    Methods:\n        process(tp_m, tp_b, conf, pred_cls, target_cls): Processes metrics over the given set of predictions.\n        mean_results(): Returns the mean of the detection and segmentation metrics over all the classes.\n        class_result(i): Returns the detection and segmentation metrics of class `i`.\n        maps: Returns the mean Average Precision (mAP) scores for IoU thresholds ranging from 0.50 to 0.95.\n        fitness: Returns the fitness scores, which are a single weighted combination of metrics.\n        ap_class_index: Returns the list of indices of classes used to compute Average Precision (AP).\n        results_dict: Returns the dictionary containing all the detection and segmentation metrics and fitness score.\n    "
     ]
    }
   ],
   "source": [
    "# VALIDATION ON DORMANT DATASET\n",
    "#Import necessary libraries\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "model_path = '/media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/Experiment/CBAM/M/train_cbam/weights/best.pt'\n",
    "data_path = '/media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/data.yaml'\n",
    "\n",
    "# Load the YOLO11n model\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# Validate the model using the new dataset\n",
    "results = model.val(data=data_path)\n",
    "\n",
    "# Print the validation metrics\n",
    "print(\"Validation Metrics for Bounding Box:\")\n",
    "print(f\"mAP50-95: {results.box.map}\")\n",
    "print(f\"mAP50: {results.box.map50}\")\n",
    "print(f\"mAP75: {results.box.map75}\")\n",
    "\n",
    "print(\"Validation Metrics for Segmentation Mask:\")\n",
    "print(f\"mAP50-95: {results.mask.map}\")\n",
    "print(f\"mAP50: {results.mask.map50}\")\n",
    "print(f\"mAP75: {results.mask.map75}\")\n",
    "\n",
    "# Assume results.speed contains the processing speed information\n",
    "print(f\"Processing Speeds: {results.speed}\")\n",
    "\n",
    "# Additional code to visualize results or save output can be added here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a099682-422a-467f-a558-eaca041d4cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.38 üöÄ Python-3.11.7 torch-2.5.1+cu124 CUDA:0 (NVIDIA TITAN Xp COLLECTORS EDITION, 12189MiB)\n",
      "YOLO11l-seg summary (fused): 491 layers, 27,586,134 parameters, 0 gradients, 141.9 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11Att\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/valid/images/tree12_00700_jpg.rf.79df4c55f73b4b9735fa1d83a09b1a92.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/valid/images/tree14_00500_jpg.rf.a4b19cb8c74fec773ca46434642db08f.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/valid/images/tree6_01000_jpg.rf.49bd8f6c55b97b57cad26a96e1ddecb9.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/valid/images/tree7_00600_jpg.rf.3fd015c4f0bc7b60680d45d2f7901404.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/valid/images/tree8_00700_jpg.rf.82d15fb1e082da1e6df9993535ed5011.jpg: 1 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è NMS time limit 2.800s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         78       3558      0.906       0.73      0.794      0.683      0.875      0.695      0.756      0.502\n",
      "                Branch         78       2651      0.891      0.677      0.755      0.602      0.841      0.622      0.685      0.376\n",
      "                 Trunk         78        907      0.921      0.783      0.834      0.765      0.908      0.767      0.827      0.628\n",
      "Speed: 24.8ms preprocess, 44.3ms inference, 0.0ms loss, 55.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val4\u001b[0m\n",
      "Validation Metrics for Bounding Box:\n",
      "mAP50-95: 0.6833035740837331\n",
      "mAP50: 0.7941674986423604\n",
      "mAP75: 0.7413296524181536\n",
      "Validation Metrics for Segmentation Mask:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SegmentMetrics' object has no attribute 'mask'. See valid attributes below.\n\n    Calculates and aggregates detection and segmentation metrics over a given set of classes.\n\n    Args:\n        save_dir (Path): Path to the directory where the output plots should be saved. Default is the current directory.\n        plot (bool): Whether to save the detection and segmentation plots. Default is False.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n        names (list): List of class names. Default is an empty list.\n\n    Attributes:\n        save_dir (Path): Path to the directory where the output plots should be saved.\n        plot (bool): Whether to save the detection and segmentation plots.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n        names (list): List of class names.\n        box (Metric): An instance of the Metric class to calculate box detection metrics.\n        seg (Metric): An instance of the Metric class to calculate mask segmentation metrics.\n        speed (dict): Dictionary to store the time taken in different phases of inference.\n\n    Methods:\n        process(tp_m, tp_b, conf, pred_cls, target_cls): Processes metrics over the given set of predictions.\n        mean_results(): Returns the mean of the detection and segmentation metrics over all the classes.\n        class_result(i): Returns the detection and segmentation metrics of class `i`.\n        maps: Returns the mean Average Precision (mAP) scores for IoU thresholds ranging from 0.50 to 0.95.\n        fitness: Returns the fitness scores, which are a single weighted combination of metrics.\n        ap_class_index: Returns the list of indices of classes used to compute Average Precision (AP).\n        results_dict: Returns the dictionary containing all the detection and segmentation metrics and fitness score.\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP75: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mbox\u001b[38;5;241m.\u001b[39mmap75\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Metrics for Segmentation Mask:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP50-95: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmap\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP50: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmap50\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP75: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmap75\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/utils/__init__.py:221\u001b[0m, in \u001b[0;36mSimpleClass.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Custom attribute access error message with helpful information.\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. See valid attributes below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SegmentMetrics' object has no attribute 'mask'. See valid attributes below.\n\n    Calculates and aggregates detection and segmentation metrics over a given set of classes.\n\n    Args:\n        save_dir (Path): Path to the directory where the output plots should be saved. Default is the current directory.\n        plot (bool): Whether to save the detection and segmentation plots. Default is False.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n        names (list): List of class names. Default is an empty list.\n\n    Attributes:\n        save_dir (Path): Path to the directory where the output plots should be saved.\n        plot (bool): Whether to save the detection and segmentation plots.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n        names (list): List of class names.\n        box (Metric): An instance of the Metric class to calculate box detection metrics.\n        seg (Metric): An instance of the Metric class to calculate mask segmentation metrics.\n        speed (dict): Dictionary to store the time taken in different phases of inference.\n\n    Methods:\n        process(tp_m, tp_b, conf, pred_cls, target_cls): Processes metrics over the given set of predictions.\n        mean_results(): Returns the mean of the detection and segmentation metrics over all the classes.\n        class_result(i): Returns the detection and segmentation metrics of class `i`.\n        maps: Returns the mean Average Precision (mAP) scores for IoU thresholds ranging from 0.50 to 0.95.\n        fitness: Returns the fitness scores, which are a single weighted combination of metrics.\n        ap_class_index: Returns the list of indices of classes used to compute Average Precision (AP).\n        results_dict: Returns the dictionary containing all the detection and segmentation metrics and fitness score.\n    "
     ]
    }
   ],
   "source": [
    "# VALIDATION ON DORMANT DATASET\n",
    "#Import necessary libraries\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "model_path = '/media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/Experiment/CBAM/L/train_cbam/weights/best.pt'\n",
    "data_path = '/media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/data.yaml'\n",
    "\n",
    "# Load the YOLO11n model\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# Validate the model using the new dataset\n",
    "results = model.val(data=data_path)\n",
    "\n",
    "# Print the validation metrics\n",
    "print(\"Validation Metrics for Bounding Box:\")\n",
    "print(f\"mAP50-95: {results.box.map}\")\n",
    "print(f\"mAP50: {results.box.map50}\")\n",
    "print(f\"mAP75: {results.box.map75}\")\n",
    "\n",
    "print(\"Validation Metrics for Segmentation Mask:\")\n",
    "print(f\"mAP50-95: {results.mask.map}\")\n",
    "print(f\"mAP50: {results.mask.map50}\")\n",
    "print(f\"mAP75: {results.mask.map75}\")\n",
    "\n",
    "# Assume results.speed contains the processing speed information\n",
    "print(f\"Processing Speeds: {results.speed}\")\n",
    "\n",
    "# Additional code to visualize results or save output can be added here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10d83ef6-4476-4d57-8f1e-2902fe632ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.38 üöÄ Python-3.11.7 torch-2.5.1+cu124 CUDA:0 (NVIDIA TITAN Xp COLLECTORS EDITION, 12189MiB)\n",
      "YOLO11x-seg summary (fused): 491 layers, 62,004,438 parameters, 0 gradients, 318.5 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11Att\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/valid/images/tree12_00700_jpg.rf.79df4c55f73b4b9735fa1d83a09b1a92.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/valid/images/tree14_00500_jpg.rf.a4b19cb8c74fec773ca46434642db08f.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/valid/images/tree6_01000_jpg.rf.49bd8f6c55b97b57cad26a96e1ddecb9.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/valid/images/tree7_00600_jpg.rf.3fd015c4f0bc7b60680d45d2f7901404.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/valid/images/tree8_00700_jpg.rf.82d15fb1e082da1e6df9993535ed5011.jpg: 1 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         78       3558      0.937      0.887      0.934      0.827       0.91      0.832      0.884      0.588\n",
      "                Branch         78       2651      0.915      0.828      0.897      0.741      0.871      0.742      0.808      0.441\n",
      "                 Trunk         78        907       0.96      0.947      0.972      0.912      0.949      0.923      0.959      0.736\n",
      "Speed: 1.6ms preprocess, 20.6ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val5\u001b[0m\n",
      "Validation Metrics for Bounding Box:\n",
      "mAP50-95: 0.8265642570859842\n",
      "mAP50: 0.9342622539012692\n",
      "mAP75: 0.8843341609780628\n",
      "Validation Metrics for Segmentation Mask:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SegmentMetrics' object has no attribute 'mask'. See valid attributes below.\n\n    Calculates and aggregates detection and segmentation metrics over a given set of classes.\n\n    Args:\n        save_dir (Path): Path to the directory where the output plots should be saved. Default is the current directory.\n        plot (bool): Whether to save the detection and segmentation plots. Default is False.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n        names (list): List of class names. Default is an empty list.\n\n    Attributes:\n        save_dir (Path): Path to the directory where the output plots should be saved.\n        plot (bool): Whether to save the detection and segmentation plots.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n        names (list): List of class names.\n        box (Metric): An instance of the Metric class to calculate box detection metrics.\n        seg (Metric): An instance of the Metric class to calculate mask segmentation metrics.\n        speed (dict): Dictionary to store the time taken in different phases of inference.\n\n    Methods:\n        process(tp_m, tp_b, conf, pred_cls, target_cls): Processes metrics over the given set of predictions.\n        mean_results(): Returns the mean of the detection and segmentation metrics over all the classes.\n        class_result(i): Returns the detection and segmentation metrics of class `i`.\n        maps: Returns the mean Average Precision (mAP) scores for IoU thresholds ranging from 0.50 to 0.95.\n        fitness: Returns the fitness scores, which are a single weighted combination of metrics.\n        ap_class_index: Returns the list of indices of classes used to compute Average Precision (AP).\n        results_dict: Returns the dictionary containing all the detection and segmentation metrics and fitness score.\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP75: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mbox\u001b[38;5;241m.\u001b[39mmap75\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Metrics for Segmentation Mask:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP50-95: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmap\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP50: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmap50\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP75: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmap75\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/utils/__init__.py:221\u001b[0m, in \u001b[0;36mSimpleClass.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Custom attribute access error message with helpful information.\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. See valid attributes below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SegmentMetrics' object has no attribute 'mask'. See valid attributes below.\n\n    Calculates and aggregates detection and segmentation metrics over a given set of classes.\n\n    Args:\n        save_dir (Path): Path to the directory where the output plots should be saved. Default is the current directory.\n        plot (bool): Whether to save the detection and segmentation plots. Default is False.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n        names (list): List of class names. Default is an empty list.\n\n    Attributes:\n        save_dir (Path): Path to the directory where the output plots should be saved.\n        plot (bool): Whether to save the detection and segmentation plots.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n        names (list): List of class names.\n        box (Metric): An instance of the Metric class to calculate box detection metrics.\n        seg (Metric): An instance of the Metric class to calculate mask segmentation metrics.\n        speed (dict): Dictionary to store the time taken in different phases of inference.\n\n    Methods:\n        process(tp_m, tp_b, conf, pred_cls, target_cls): Processes metrics over the given set of predictions.\n        mean_results(): Returns the mean of the detection and segmentation metrics over all the classes.\n        class_result(i): Returns the detection and segmentation metrics of class `i`.\n        maps: Returns the mean Average Precision (mAP) scores for IoU thresholds ranging from 0.50 to 0.95.\n        fitness: Returns the fitness scores, which are a single weighted combination of metrics.\n        ap_class_index: Returns the list of indices of classes used to compute Average Precision (AP).\n        results_dict: Returns the dictionary containing all the detection and segmentation metrics and fitness score.\n    "
     ]
    }
   ],
   "source": [
    "# VALIDATION ON DORMANT DATASET\n",
    "#Import necessary libraries\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "model_path = '/media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/Experiment/CBAM/X/train_cbam/weights/best.pt'\n",
    "data_path = '/media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Dormant/Dormant.v2i.yolov11/data.yaml'\n",
    "\n",
    "# Load the YOLO11n model\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# Validate the model using the new dataset\n",
    "results = model.val(data=data_path)\n",
    "\n",
    "# Print the validation metrics\n",
    "print(\"Validation Metrics for Bounding Box:\")\n",
    "print(f\"mAP50-95: {results.box.map}\")\n",
    "print(f\"mAP50: {results.box.map50}\")\n",
    "print(f\"mAP75: {results.box.map75}\")\n",
    "\n",
    "print(\"Validation Metrics for Segmentation Mask:\")\n",
    "print(f\"mAP50-95: {results.mask.map}\")\n",
    "print(f\"mAP50: {results.mask.map50}\")\n",
    "print(f\"mAP75: {results.mask.map75}\")\n",
    "\n",
    "# Assume results.speed contains the processing speed information\n",
    "print(f\"Processing Speeds: {results.speed}\")\n",
    "\n",
    "# Additional code to visualize results or save output can be added here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb89a746-c606-4e59-8510-c0beab9aad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATION ON DORMANT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cdcabb9-a687-4987-a348-4ffbbe0a9e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.38 üöÄ Python-3.11.7 torch-2.5.1+cu124 CUDA:0 (NVIDIA TITAN Xp COLLECTORS EDITION, 12189MiB)\n",
      "YOLO11n-seg summary (fused): 265 layers, 2,834,958 parameters, 0 gradients, 10.2 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11Att\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Canopy/Canopy.v1i.yolov11 (1)/valid/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         78       1737      0.541      0.459      0.441      0.211      0.484      0.416      0.382      0.162\n",
      "                Branch         78       1066      0.517      0.399      0.369      0.136      0.422      0.326      0.284     0.0802\n",
      "                 Trunk         78        671      0.565       0.52      0.513      0.285      0.546      0.505      0.479      0.244\n",
      "Speed: 12.4ms preprocess, 5.2ms inference, 0.0ms loss, 2.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val6\u001b[0m\n",
      "Validation Metrics for Bounding Box:\n",
      "mAP50-95: 0.21062686748110596\n",
      "mAP50: 0.4408732852205558\n",
      "mAP75: 0.18019373414310313\n",
      "Validation Metrics for Segmentation Mask:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SegmentMetrics' object has no attribute 'mask'. See valid attributes below.\n\n    Calculates and aggregates detection and segmentation metrics over a given set of classes.\n\n    Args:\n        save_dir (Path): Path to the directory where the output plots should be saved. Default is the current directory.\n        plot (bool): Whether to save the detection and segmentation plots. Default is False.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n        names (list): List of class names. Default is an empty list.\n\n    Attributes:\n        save_dir (Path): Path to the directory where the output plots should be saved.\n        plot (bool): Whether to save the detection and segmentation plots.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n        names (list): List of class names.\n        box (Metric): An instance of the Metric class to calculate box detection metrics.\n        seg (Metric): An instance of the Metric class to calculate mask segmentation metrics.\n        speed (dict): Dictionary to store the time taken in different phases of inference.\n\n    Methods:\n        process(tp_m, tp_b, conf, pred_cls, target_cls): Processes metrics over the given set of predictions.\n        mean_results(): Returns the mean of the detection and segmentation metrics over all the classes.\n        class_result(i): Returns the detection and segmentation metrics of class `i`.\n        maps: Returns the mean Average Precision (mAP) scores for IoU thresholds ranging from 0.50 to 0.95.\n        fitness: Returns the fitness scores, which are a single weighted combination of metrics.\n        ap_class_index: Returns the list of indices of classes used to compute Average Precision (AP).\n        results_dict: Returns the dictionary containing all the detection and segmentation metrics and fitness score.\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP75: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mbox\u001b[38;5;241m.\u001b[39mmap75\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Metrics for Segmentation Mask:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP50-95: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmap\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP50: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmap50\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP75: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmap75\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/utils/__init__.py:221\u001b[0m, in \u001b[0;36mSimpleClass.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Custom attribute access error message with helpful information.\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. See valid attributes below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SegmentMetrics' object has no attribute 'mask'. See valid attributes below.\n\n    Calculates and aggregates detection and segmentation metrics over a given set of classes.\n\n    Args:\n        save_dir (Path): Path to the directory where the output plots should be saved. Default is the current directory.\n        plot (bool): Whether to save the detection and segmentation plots. Default is False.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n        names (list): List of class names. Default is an empty list.\n\n    Attributes:\n        save_dir (Path): Path to the directory where the output plots should be saved.\n        plot (bool): Whether to save the detection and segmentation plots.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n        names (list): List of class names.\n        box (Metric): An instance of the Metric class to calculate box detection metrics.\n        seg (Metric): An instance of the Metric class to calculate mask segmentation metrics.\n        speed (dict): Dictionary to store the time taken in different phases of inference.\n\n    Methods:\n        process(tp_m, tp_b, conf, pred_cls, target_cls): Processes metrics over the given set of predictions.\n        mean_results(): Returns the mean of the detection and segmentation metrics over all the classes.\n        class_result(i): Returns the detection and segmentation metrics of class `i`.\n        maps: Returns the mean Average Precision (mAP) scores for IoU thresholds ranging from 0.50 to 0.95.\n        fitness: Returns the fitness scores, which are a single weighted combination of metrics.\n        ap_class_index: Returns the list of indices of classes used to compute Average Precision (AP).\n        results_dict: Returns the dictionary containing all the detection and segmentation metrics and fitness score.\n    "
     ]
    }
   ],
   "source": [
    "# VALIDATION ON DORMANT DATASET\n",
    "# VALIDATION ON DORMANT DATASET\n",
    "#Import necessary libraries\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "model_path = '/media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/Experiment/CBAM/N/train_cbam/weights/best.pt'\n",
    "data_path = '/media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Canopy/Canopy.v1i.yolov11 (1)/data.yaml'\n",
    "\n",
    "# Load the YOLO11n model\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# Validate the model using the new dataset\n",
    "results = model.val(data=data_path)\n",
    "\n",
    "# Print the validation metrics\n",
    "print(\"Validation Metrics for Bounding Box:\")\n",
    "print(f\"mAP50-95: {results.box.map}\")\n",
    "print(f\"mAP50: {results.box.map50}\")\n",
    "print(f\"mAP75: {results.box.map75}\")\n",
    "\n",
    "print(\"Validation Metrics for Segmentation Mask:\")\n",
    "print(f\"mAP50-95: {results.mask.map}\")\n",
    "print(f\"mAP50: {results.mask.map50}\")\n",
    "print(f\"mAP75: {results.mask.map75}\")\n",
    "\n",
    "# Assume results.speed contains the processing speed information\n",
    "print(f\"Processing Speeds: {results.speed}\")\n",
    "\n",
    "# Additional code to visualize results or save output can be added here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8483aca9-8ccd-4af9-81c9-31707845a892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.38 üöÄ Python-3.11.7 torch-2.5.1+cu124 CUDA:0 (NVIDIA TITAN Xp COLLECTORS EDITION, 12189MiB)\n",
      "YOLO11s-seg summary (fused): 265 layers, 10,067,590 parameters, 0 gradients, 35.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11Att\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         78       1737      0.671      0.476      0.524      0.273      0.579      0.416      0.435      0.196\n",
      "                Branch         78       1066      0.645      0.421      0.466      0.199      0.516      0.342      0.345       0.11\n",
      "                 Trunk         78        671      0.697      0.531      0.582      0.347      0.643       0.49      0.525      0.283\n",
      "Speed: 6.4ms preprocess, 16.9ms inference, 0.0ms loss, 4.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val7\u001b[0m\n",
      "Validation Metrics for Bounding Box:\n",
      "mAP50-95: 0.27266488007964146\n",
      "mAP50: 0.5235428200568448\n",
      "mAP75: 0.2502350781129199\n",
      "Validation Metrics for Segmentation Mask:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SegmentMetrics' object has no attribute 'mask'. See valid attributes below.\n\n    Calculates and aggregates detection and segmentation metrics over a given set of classes.\n\n    Args:\n        save_dir (Path): Path to the directory where the output plots should be saved. Default is the current directory.\n        plot (bool): Whether to save the detection and segmentation plots. Default is False.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n        names (list): List of class names. Default is an empty list.\n\n    Attributes:\n        save_dir (Path): Path to the directory where the output plots should be saved.\n        plot (bool): Whether to save the detection and segmentation plots.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n        names (list): List of class names.\n        box (Metric): An instance of the Metric class to calculate box detection metrics.\n        seg (Metric): An instance of the Metric class to calculate mask segmentation metrics.\n        speed (dict): Dictionary to store the time taken in different phases of inference.\n\n    Methods:\n        process(tp_m, tp_b, conf, pred_cls, target_cls): Processes metrics over the given set of predictions.\n        mean_results(): Returns the mean of the detection and segmentation metrics over all the classes.\n        class_result(i): Returns the detection and segmentation metrics of class `i`.\n        maps: Returns the mean Average Precision (mAP) scores for IoU thresholds ranging from 0.50 to 0.95.\n        fitness: Returns the fitness scores, which are a single weighted combination of metrics.\n        ap_class_index: Returns the list of indices of classes used to compute Average Precision (AP).\n        results_dict: Returns the dictionary containing all the detection and segmentation metrics and fitness score.\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP75: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mbox\u001b[38;5;241m.\u001b[39mmap75\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Metrics for Segmentation Mask:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP50-95: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmap\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP50: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmap50\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP75: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmap75\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/utils/__init__.py:221\u001b[0m, in \u001b[0;36mSimpleClass.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Custom attribute access error message with helpful information.\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. See valid attributes below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SegmentMetrics' object has no attribute 'mask'. See valid attributes below.\n\n    Calculates and aggregates detection and segmentation metrics over a given set of classes.\n\n    Args:\n        save_dir (Path): Path to the directory where the output plots should be saved. Default is the current directory.\n        plot (bool): Whether to save the detection and segmentation plots. Default is False.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n        names (list): List of class names. Default is an empty list.\n\n    Attributes:\n        save_dir (Path): Path to the directory where the output plots should be saved.\n        plot (bool): Whether to save the detection and segmentation plots.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n        names (list): List of class names.\n        box (Metric): An instance of the Metric class to calculate box detection metrics.\n        seg (Metric): An instance of the Metric class to calculate mask segmentation metrics.\n        speed (dict): Dictionary to store the time taken in different phases of inference.\n\n    Methods:\n        process(tp_m, tp_b, conf, pred_cls, target_cls): Processes metrics over the given set of predictions.\n        mean_results(): Returns the mean of the detection and segmentation metrics over all the classes.\n        class_result(i): Returns the detection and segmentation metrics of class `i`.\n        maps: Returns the mean Average Precision (mAP) scores for IoU thresholds ranging from 0.50 to 0.95.\n        fitness: Returns the fitness scores, which are a single weighted combination of metrics.\n        ap_class_index: Returns the list of indices of classes used to compute Average Precision (AP).\n        results_dict: Returns the dictionary containing all the detection and segmentation metrics and fitness score.\n    "
     ]
    }
   ],
   "source": [
    "# VALIDATION ON DORMANT DATASET\n",
    "# VALIDATION ON DORMANT DATASET\n",
    "#Import necessary libraries\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "model_path = '/media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/Experiment/CBAM/S/train_cbam/weights/best.pt'\n",
    "data_path = '/media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Canopy/Canopy.v1i.yolov11 (1)/data.yaml'\n",
    "\n",
    "# Load the YOLO11n model\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# Validate the model using the new dataset\n",
    "results = model.val(data=data_path)\n",
    "\n",
    "# Print the validation metrics\n",
    "print(\"Validation Metrics for Bounding Box:\")\n",
    "print(f\"mAP50-95: {results.box.map}\")\n",
    "print(f\"mAP50: {results.box.map50}\")\n",
    "print(f\"mAP75: {results.box.map75}\")\n",
    "\n",
    "print(\"Validation Metrics for Segmentation Mask:\")\n",
    "print(f\"mAP50-95: {results.mask.map}\")\n",
    "print(f\"mAP50: {results.mask.map50}\")\n",
    "print(f\"mAP75: {results.mask.map75}\")\n",
    "\n",
    "# Assume results.speed contains the processing speed information\n",
    "print(f\"Processing Speeds: {results.speed}\")\n",
    "\n",
    "# Additional code to visualize results or save output can be added here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f22d2e8-0b34-4713-ab29-765982941e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.38 üöÄ Python-3.11.7 torch-2.5.1+cu124 CUDA:0 (NVIDIA TITAN Xp COLLECTORS EDITION, 12189MiB)\n",
      "YOLO11m-seg summary (fused): 330 layers, 22,336,854 parameters, 0 gradients, 123.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11Att\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è NMS time limit 2.800s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         78       1737       0.54      0.332      0.366      0.196      0.501      0.292      0.319      0.143\n",
      "                Branch         78       1066      0.538      0.285      0.337      0.149      0.448       0.22      0.253     0.0764\n",
      "                 Trunk         78        671      0.542      0.379      0.395      0.243      0.554      0.365      0.386      0.211\n",
      "Speed: 4.3ms preprocess, 91.8ms inference, 0.0ms loss, 78.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val8\u001b[0m\n",
      "Validation Metrics for Bounding Box:\n",
      "mAP50-95: 0.19573278127406168\n",
      "mAP50: 0.3661356704354276\n",
      "mAP75: 0.18643485677892735\n",
      "Validation Metrics for Segmentation Mask:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SegmentMetrics' object has no attribute 'mask'. See valid attributes below.\n\n    Calculates and aggregates detection and segmentation metrics over a given set of classes.\n\n    Args:\n        save_dir (Path): Path to the directory where the output plots should be saved. Default is the current directory.\n        plot (bool): Whether to save the detection and segmentation plots. Default is False.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n        names (list): List of class names. Default is an empty list.\n\n    Attributes:\n        save_dir (Path): Path to the directory where the output plots should be saved.\n        plot (bool): Whether to save the detection and segmentation plots.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n        names (list): List of class names.\n        box (Metric): An instance of the Metric class to calculate box detection metrics.\n        seg (Metric): An instance of the Metric class to calculate mask segmentation metrics.\n        speed (dict): Dictionary to store the time taken in different phases of inference.\n\n    Methods:\n        process(tp_m, tp_b, conf, pred_cls, target_cls): Processes metrics over the given set of predictions.\n        mean_results(): Returns the mean of the detection and segmentation metrics over all the classes.\n        class_result(i): Returns the detection and segmentation metrics of class `i`.\n        maps: Returns the mean Average Precision (mAP) scores for IoU thresholds ranging from 0.50 to 0.95.\n        fitness: Returns the fitness scores, which are a single weighted combination of metrics.\n        ap_class_index: Returns the list of indices of classes used to compute Average Precision (AP).\n        results_dict: Returns the dictionary containing all the detection and segmentation metrics and fitness score.\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP75: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mbox\u001b[38;5;241m.\u001b[39mmap75\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Metrics for Segmentation Mask:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP50-95: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmap\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP50: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmap50\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP75: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmap75\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/utils/__init__.py:221\u001b[0m, in \u001b[0;36mSimpleClass.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Custom attribute access error message with helpful information.\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. See valid attributes below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SegmentMetrics' object has no attribute 'mask'. See valid attributes below.\n\n    Calculates and aggregates detection and segmentation metrics over a given set of classes.\n\n    Args:\n        save_dir (Path): Path to the directory where the output plots should be saved. Default is the current directory.\n        plot (bool): Whether to save the detection and segmentation plots. Default is False.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n        names (list): List of class names. Default is an empty list.\n\n    Attributes:\n        save_dir (Path): Path to the directory where the output plots should be saved.\n        plot (bool): Whether to save the detection and segmentation plots.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n        names (list): List of class names.\n        box (Metric): An instance of the Metric class to calculate box detection metrics.\n        seg (Metric): An instance of the Metric class to calculate mask segmentation metrics.\n        speed (dict): Dictionary to store the time taken in different phases of inference.\n\n    Methods:\n        process(tp_m, tp_b, conf, pred_cls, target_cls): Processes metrics over the given set of predictions.\n        mean_results(): Returns the mean of the detection and segmentation metrics over all the classes.\n        class_result(i): Returns the detection and segmentation metrics of class `i`.\n        maps: Returns the mean Average Precision (mAP) scores for IoU thresholds ranging from 0.50 to 0.95.\n        fitness: Returns the fitness scores, which are a single weighted combination of metrics.\n        ap_class_index: Returns the list of indices of classes used to compute Average Precision (AP).\n        results_dict: Returns the dictionary containing all the detection and segmentation metrics and fitness score.\n    "
     ]
    }
   ],
   "source": [
    "# VALIDATION ON DORMANT DATASET\n",
    "# VALIDATION ON DORMANT DATASET\n",
    "#Import necessary libraries\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "model_path = '/media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/Experiment/CBAM/M/train_cbam/weights/best.pt'\n",
    "data_path = '/media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Canopy/Canopy.v1i.yolov11 (1)/data.yaml'\n",
    "\n",
    "# Load the YOLO11n model\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# Validate the model using the new dataset\n",
    "results = model.val(data=data_path)\n",
    "\n",
    "# Print the validation metrics\n",
    "print(\"Validation Metrics for Bounding Box:\")\n",
    "print(f\"mAP50-95: {results.box.map}\")\n",
    "print(f\"mAP50: {results.box.map50}\")\n",
    "print(f\"mAP75: {results.box.map75}\")\n",
    "\n",
    "print(\"Validation Metrics for Segmentation Mask:\")\n",
    "print(f\"mAP50-95: {results.mask.map}\")\n",
    "print(f\"mAP50: {results.mask.map50}\")\n",
    "print(f\"mAP75: {results.mask.map75}\")\n",
    "\n",
    "# Assume results.speed contains the processing speed information\n",
    "print(f\"Processing Speeds: {results.speed}\")\n",
    "\n",
    "# Additional code to visualize results or save output can be added here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40ee51c5-b939-42a0-8012-fe91db1220c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.38 üöÄ Python-3.11.7 torch-2.5.1+cu124 CUDA:0 (NVIDIA TITAN Xp COLLECTORS EDITION, 12189MiB)\n",
      "YOLO11l-seg summary (fused): 491 layers, 27,586,134 parameters, 0 gradients, 141.9 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11Att\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         78       1737      0.625      0.415      0.468      0.243      0.546      0.358      0.385      0.157\n",
      "                Branch         78       1066      0.635       0.37      0.444      0.189      0.503      0.287      0.317     0.0928\n",
      "                 Trunk         78        671      0.615       0.46      0.491      0.297      0.589      0.429      0.453      0.222\n",
      "Speed: 3.0ms preprocess, 19.3ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val9\u001b[0m\n",
      "Validation Metrics for Bounding Box:\n",
      "mAP50-95: 0.24326472686751974\n",
      "mAP50: 0.46753579306883897\n",
      "mAP75: 0.22638218258634885\n",
      "Validation Metrics for Segmentation Mask:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SegmentMetrics' object has no attribute 'mask'. See valid attributes below.\n\n    Calculates and aggregates detection and segmentation metrics over a given set of classes.\n\n    Args:\n        save_dir (Path): Path to the directory where the output plots should be saved. Default is the current directory.\n        plot (bool): Whether to save the detection and segmentation plots. Default is False.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n        names (list): List of class names. Default is an empty list.\n\n    Attributes:\n        save_dir (Path): Path to the directory where the output plots should be saved.\n        plot (bool): Whether to save the detection and segmentation plots.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n        names (list): List of class names.\n        box (Metric): An instance of the Metric class to calculate box detection metrics.\n        seg (Metric): An instance of the Metric class to calculate mask segmentation metrics.\n        speed (dict): Dictionary to store the time taken in different phases of inference.\n\n    Methods:\n        process(tp_m, tp_b, conf, pred_cls, target_cls): Processes metrics over the given set of predictions.\n        mean_results(): Returns the mean of the detection and segmentation metrics over all the classes.\n        class_result(i): Returns the detection and segmentation metrics of class `i`.\n        maps: Returns the mean Average Precision (mAP) scores for IoU thresholds ranging from 0.50 to 0.95.\n        fitness: Returns the fitness scores, which are a single weighted combination of metrics.\n        ap_class_index: Returns the list of indices of classes used to compute Average Precision (AP).\n        results_dict: Returns the dictionary containing all the detection and segmentation metrics and fitness score.\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP75: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mbox\u001b[38;5;241m.\u001b[39mmap75\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Metrics for Segmentation Mask:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP50-95: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmap\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP50: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmap50\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP75: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmap75\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/utils/__init__.py:221\u001b[0m, in \u001b[0;36mSimpleClass.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Custom attribute access error message with helpful information.\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. See valid attributes below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SegmentMetrics' object has no attribute 'mask'. See valid attributes below.\n\n    Calculates and aggregates detection and segmentation metrics over a given set of classes.\n\n    Args:\n        save_dir (Path): Path to the directory where the output plots should be saved. Default is the current directory.\n        plot (bool): Whether to save the detection and segmentation plots. Default is False.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n        names (list): List of class names. Default is an empty list.\n\n    Attributes:\n        save_dir (Path): Path to the directory where the output plots should be saved.\n        plot (bool): Whether to save the detection and segmentation plots.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n        names (list): List of class names.\n        box (Metric): An instance of the Metric class to calculate box detection metrics.\n        seg (Metric): An instance of the Metric class to calculate mask segmentation metrics.\n        speed (dict): Dictionary to store the time taken in different phases of inference.\n\n    Methods:\n        process(tp_m, tp_b, conf, pred_cls, target_cls): Processes metrics over the given set of predictions.\n        mean_results(): Returns the mean of the detection and segmentation metrics over all the classes.\n        class_result(i): Returns the detection and segmentation metrics of class `i`.\n        maps: Returns the mean Average Precision (mAP) scores for IoU thresholds ranging from 0.50 to 0.95.\n        fitness: Returns the fitness scores, which are a single weighted combination of metrics.\n        ap_class_index: Returns the list of indices of classes used to compute Average Precision (AP).\n        results_dict: Returns the dictionary containing all the detection and segmentation metrics and fitness score.\n    "
     ]
    }
   ],
   "source": [
    "# VALIDATION ON DORMANT DATASET\n",
    "# VALIDATION ON DORMANT DATASET\n",
    "#Import necessary libraries\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "model_path = '/media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/Experiment/CBAM/L/train_cbam/weights/best.pt'\n",
    "data_path = '/media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Canopy/Canopy.v1i.yolov11 (1)/data.yaml'\n",
    "\n",
    "# Load the YOLO11n model\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# Validate the model using the new dataset\n",
    "results = model.val(data=data_path)\n",
    "\n",
    "# Print the validation metrics\n",
    "print(\"Validation Metrics for Bounding Box:\")\n",
    "print(f\"mAP50-95: {results.box.map}\")\n",
    "print(f\"mAP50: {results.box.map50}\")\n",
    "print(f\"mAP75: {results.box.map75}\")\n",
    "\n",
    "print(\"Validation Metrics for Segmentation Mask:\")\n",
    "print(f\"mAP50-95: {results.mask.map}\")\n",
    "print(f\"mAP50: {results.mask.map50}\")\n",
    "print(f\"mAP75: {results.mask.map75}\")\n",
    "\n",
    "# Assume results.speed contains the processing speed information\n",
    "print(f\"Processing Speeds: {results.speed}\")\n",
    "\n",
    "# Additional code to visualize results or save output can be added here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d894a7a5-4372-4af1-b0d5-5ca99e789ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.38 üöÄ Python-3.11.7 torch-2.5.1+cu124 CUDA:0 (NVIDIA TITAN Xp COLLECTORS EDITION, 12189MiB)\n",
      "YOLO11x-seg summary (fused): 491 layers, 62,004,438 parameters, 0 gradients, 318.5 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11Att\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         78       1737      0.575      0.475       0.51      0.273      0.569      0.378      0.415       0.18\n",
      "                Branch         78       1066       0.55      0.422       0.46      0.199      0.523      0.302      0.329     0.0993\n",
      "                 Trunk         78        671      0.601      0.529      0.559      0.346      0.615      0.454      0.501      0.262\n",
      "Speed: 2.5ms preprocess, 36.3ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val10\u001b[0m\n",
      "Validation Metrics for Bounding Box:\n",
      "mAP50-95: 0.27275962594224784\n",
      "mAP50: 0.5095414418062147\n",
      "mAP75: 0.25796034069763263\n",
      "Validation Metrics for Segmentation Mask:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SegmentMetrics' object has no attribute 'mask'. See valid attributes below.\n\n    Calculates and aggregates detection and segmentation metrics over a given set of classes.\n\n    Args:\n        save_dir (Path): Path to the directory where the output plots should be saved. Default is the current directory.\n        plot (bool): Whether to save the detection and segmentation plots. Default is False.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n        names (list): List of class names. Default is an empty list.\n\n    Attributes:\n        save_dir (Path): Path to the directory where the output plots should be saved.\n        plot (bool): Whether to save the detection and segmentation plots.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n        names (list): List of class names.\n        box (Metric): An instance of the Metric class to calculate box detection metrics.\n        seg (Metric): An instance of the Metric class to calculate mask segmentation metrics.\n        speed (dict): Dictionary to store the time taken in different phases of inference.\n\n    Methods:\n        process(tp_m, tp_b, conf, pred_cls, target_cls): Processes metrics over the given set of predictions.\n        mean_results(): Returns the mean of the detection and segmentation metrics over all the classes.\n        class_result(i): Returns the detection and segmentation metrics of class `i`.\n        maps: Returns the mean Average Precision (mAP) scores for IoU thresholds ranging from 0.50 to 0.95.\n        fitness: Returns the fitness scores, which are a single weighted combination of metrics.\n        ap_class_index: Returns the list of indices of classes used to compute Average Precision (AP).\n        results_dict: Returns the dictionary containing all the detection and segmentation metrics and fitness score.\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP75: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mbox\u001b[38;5;241m.\u001b[39mmap75\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Metrics for Segmentation Mask:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP50-95: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmap\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP50: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmap50\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP75: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmap75\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/utils/__init__.py:221\u001b[0m, in \u001b[0;36mSimpleClass.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Custom attribute access error message with helpful information.\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. See valid attributes below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SegmentMetrics' object has no attribute 'mask'. See valid attributes below.\n\n    Calculates and aggregates detection and segmentation metrics over a given set of classes.\n\n    Args:\n        save_dir (Path): Path to the directory where the output plots should be saved. Default is the current directory.\n        plot (bool): Whether to save the detection and segmentation plots. Default is False.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n        names (list): List of class names. Default is an empty list.\n\n    Attributes:\n        save_dir (Path): Path to the directory where the output plots should be saved.\n        plot (bool): Whether to save the detection and segmentation plots.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n        names (list): List of class names.\n        box (Metric): An instance of the Metric class to calculate box detection metrics.\n        seg (Metric): An instance of the Metric class to calculate mask segmentation metrics.\n        speed (dict): Dictionary to store the time taken in different phases of inference.\n\n    Methods:\n        process(tp_m, tp_b, conf, pred_cls, target_cls): Processes metrics over the given set of predictions.\n        mean_results(): Returns the mean of the detection and segmentation metrics over all the classes.\n        class_result(i): Returns the detection and segmentation metrics of class `i`.\n        maps: Returns the mean Average Precision (mAP) scores for IoU thresholds ranging from 0.50 to 0.95.\n        fitness: Returns the fitness scores, which are a single weighted combination of metrics.\n        ap_class_index: Returns the list of indices of classes used to compute Average Precision (AP).\n        results_dict: Returns the dictionary containing all the detection and segmentation metrics and fitness score.\n    "
     ]
    }
   ],
   "source": [
    "# VALIDATION ON DORMANT DATASET\n",
    "# VALIDATION ON DORMANT DATASET\n",
    "#Import necessary libraries\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "model_path = '/media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/Experiment/CBAM/X/train_cbam/weights/best.pt'\n",
    "data_path = '/media/mzc/e4fcae9b-7400-4641-a800-034f249f8d14/mzc/TreesYOLO11AttentionMODULE/data/Validation/Canopy/Canopy.v1i.yolov11 (1)/data.yaml'\n",
    "\n",
    "# Load the YOLO11n model\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# Validate the model using the new dataset\n",
    "results = model.val(data=data_path)\n",
    "\n",
    "# Print the validation metrics\n",
    "print(\"Validation Metrics for Bounding Box:\")\n",
    "print(f\"mAP50-95: {results.box.map}\")\n",
    "print(f\"mAP50: {results.box.map50}\")\n",
    "print(f\"mAP75: {results.box.map75}\")\n",
    "\n",
    "print(\"Validation Metrics for Segmentation Mask:\")\n",
    "print(f\"mAP50-95: {results.mask.map}\")\n",
    "print(f\"mAP50: {results.mask.map50}\")\n",
    "print(f\"mAP75: {results.mask.map75}\")\n",
    "\n",
    "# Assume results.speed contains the processing speed information\n",
    "print(f\"Processing Speeds: {results.speed}\")\n",
    "\n",
    "# Additional code to visualize results or save output can be added here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82ad600-643e-420a-87c7-24af8e4b25f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
